model_params:
  max_len: 100  # ref: http://nlp.seas.harvard.edu/2018/04/03/attention.html
  dim_ff: 2048
  dim_model: 512
  dim_q: 64
  dim_k: 64
  dim_v: 64
  num_heads: 8
  num_encoder_layer: 6
  num_decoder_layer: 6
  dropout: 0.1

data_params:
    num_workers: 8

train_hparams:
    batch_size: 12000
    warmup_steps: 4000
    optimizer: adam
    beta_1: 0.9  # activates when optimizer is Adam
    beta_2: 0.98  # activates when optimizer is Adam
    eps: 1e-9  # FIXME: LayerNorm eps?
    steps: 100000
