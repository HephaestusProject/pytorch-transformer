max_len: 100  # ref: http://nlp.seas.harvard.edu/2018/04/03/attention.html
dim_ff: 2048
dim_model: 512
dim_q: 512
dim_k: 512
dim_v: 512
num_head: 8
num_encoder_layer: 6
num_decoder_layer: 6
residual_dropout: 0.1

data_params:
    num_workers: 8

train_hparams:
    batch_size: 12000
    warmup_steps: 4000
    optimizer: adam
    beta_1: 0.9  # activates when optimizer is Adam
    beta_2: 0.98  # activates when optimizer is Adam
    eps: 1e-9
    steps: 300000  # big
    steps: 100000
