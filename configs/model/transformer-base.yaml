model_params:
  max_len: 100  # ref: http://nlp.seas.harvard.edu/2018/04/03/attention.html
  dim_ff: 2048
  dim_model: 512
  dim_q: 64
  dim_k: 64
  dim_v: 64
  num_heads: 8
  num_encoder_layer: 6
  num_decoder_layer: 6
  dropout: 0.1

data_params:
  num_workers: 8

train_hparams:
  batch_size: 6250  # by num_tokens, paper: 25000 tokens
  steps: 400000  # paper: 100000, increase as batch_size decrease
  warmup_steps: 4000
  optimizer: adam
  beta_1: 0.9  # activates when optimizer is Adam
  beta_2: 0.98  # activates when optimizer is Adam
  eps: 1e-9  # activates when optimizer is Adam
